# -*- coding: utf-8 -*-
"""Google3D_VeronicaZhao_121024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dVslP6HkD2ufOtAVMFmUryaY1knZ-maB

# Part I

## Setup and Data Preprocessing
### 1. Set up project environment and create project documents.
### 2. Initial exploration of the dataset: identify and document missing values, outliers, and anomalies.
### 3. Data cleaning and transform data in Jupyter Notebook.

### Set up project environment and create project documents.
- Local environment is Jupyter Notebook.
- Cloud environment is Google Colab: [Colab Link](https://colab.research.google.com/drive/1cVB57dHD5CbpfPgP-XYBUL5nEoHCuiz2?usp=sharing)
- Project code maintained via GitHub: [Repo Link](https://github.com/rishitadhalbisoi/google3D)
- Project documents on Google Drive: [Drive Link](https://drive.google.com/drive/folders/1pNN3CvPdAQdRNCBkhGE_OcEM6hFn15vm?usp=sharing)
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.ensemble import IsolationForest
from collections import Counter
import re
import nltk
from nltk.corpus import stopwords

from google.colab import drive
drive.mount('/content/drive')

"""### Initial exploration of the dataset."""

# Load dataset

dataset = '/content/drive/My Drive/Google3D/US_YT_Trending_Data.csv'
df = pd.read_csv(dataset)
df

# Dataset size
print(f'{df.shape[0]} rows')
print(f'{df.shape[1]} columns')

# Display first five rows

df.head()

# Display column names and corresponding data types

df.dtypes

# object- likely a string value

# Summary statistics for numerical data

df.describe()

# Summary statistics for categorical data

df.describe(include=['object'])

# Look at the number of unique channel ids = 8302
# Videos from these channels may be more likely to go viral

df['channelId'].nunique()

df['view_count'].describe()

"""### Identify and document missing values, outliers, and anomalies."""

# Check for missing values

missing_values = df.isnull().sum()
missing_percentage = df.isnull().mean() * 100

# Document missing data

missing_df = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage': missing_percentage
})

# Display columns with missing data- only the description column has null values

missing_df = missing_df[missing_df['Missing Values'] > 0]
missing_df

numerical_cols_to_check = df.select_dtypes(include=[np.number]).columns

# Calculate IQR to identify outliers

def calculate_iqr_outliers(param_df):
    outliers_mask = pd.DataFrame(False, index=param_df.index, columns=param_df.columns)

    for column in param_df.columns:
        Q1 = param_df[column].quantile(0.25)
        Q3 = param_df[column].quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers_mask[column] = (param_df[column] < lower_bound) | (param_df[column] > upper_bound)

    outliers_any_column = outliers_mask.any(axis=1)
    outliers_df = param_df[outliers_any_column]

    return outliers_df

outliers_df = calculate_iqr_outliers(df[numerical_cols_to_check])
outliers_df['video_id'] = df.loc[outliers_df.index, 'video_id'].values
outliers_df = outliers_df.reset_index(drop=True)
outliers_df

# 78827 rows have outliers in their numerical data

# Identifying anomalies via Z-score

z_scores = np.abs(stats.zscore(df[numerical_cols_to_check]))
z_anomalies_df = df[(z_scores > 3).any(axis=1)]
z_anomalies_df

# Z-score detection yields 4941 anomalies

# Identifying anomalies using Isolation Forest

iso_forest = IsolationForest(contamination=0.1)
df['if_anomaly'] = iso_forest.fit_predict(df[numerical_cols_to_check])

if_anomalies_df = df[df['if_anomaly'] == -1]
df = df.drop(['if_anomaly'], axis=1)

if_anomalies_df = if_anomalies_df.reset_index(drop=True)

print(if_anomalies_df.loc[if_anomalies_df['view_count'].idxmax()])

if_anomalies_df

# IsolationForest yields 26416 anomalies

"""### 3. Data cleaning and transform data in Jupyter Notebook."""

# Converting publishedAt and trending_date to Datetime format

df['publishedAt'] = pd.to_datetime(df['publishedAt']) # publishedAt has a date and time
df['trending_date'] = pd.to_datetime(df['trending_date']) # trending date has no associated time
df[['publishedAt', 'trending_date']].head()

# Feature engineering year, month, day, time

df['publishedAt_year'] = df['publishedAt'].dt.year
df['publishedAt_month'] = df['publishedAt'].dt.month
df['publishedAt_date'] = df['publishedAt'].dt.day
df['publishedAt_time'] = df['publishedAt'].dt.time

df['trendingDate_year'] = df['trending_date'].dt.year
df['trendingDate_month'] = df['trending_date'].dt.month
df['trendingDate_date'] = df['trending_date'].dt.day

# Display the first few rows to check the new columns
df[['publishedAt_year', 'publishedAt_month', 'publishedAt_date', 'publishedAt_time',
          'trendingDate_year', 'trendingDate_month', 'trendingDate_date']]

# Handling missing values
# Above analysis shows the only categorical column with null values is the description column
# We can either DROP those values or fill with an empty value ''

df.dropna(subset=['description'], inplace=True)
df = df.reset_index(drop=True)
df.isnull().sum()

# Add human readable category_name column

category_mapping = {
    1:'Film & Animation',
    2:'Autos & Vehicles',
    10:'Music',
    15:'Music',
    17:'Sports',
    19:'Travel & Events',
    20:'Gaming',
    22:'Videoblogging',
    23:'Comedy',
    24:'Entertainment',
    25:'News & Politics',
    26:'Howto & Style',
    27:'Education',
    28:'Science & Technology',
    29:'Nonprofits & Activism'
}

df.insert(df.columns.get_loc('categoryId'), 'category_name', df['categoryId'].map(category_mapping))
df

# Check for duplicates and drop duplicates

duplicates = df.duplicated().sum()
print(f'{duplicates} duplicates')

df = df.drop_duplicates()
df = df.reset_index(drop=True)
df

# count how many times a video_id occurs, see how engagement rate changes
# number of unique channels: famous channels show up often, the same video may show up often

# Convert boolean columns to numerical

df['comments_disabled'] = df['comments_disabled'].astype(int)
df['ratings_disabled'] = df['ratings_disabled'].astype(int)
df[['comments_disabled', 'ratings_disabled']]

df['comments_disabled'].value_counts()

df['ratings_disabled'].value_counts()

"""### What to do about outliers/anomalies?

## Exploratory Data Analysis

### Note: data has NOT been normalized.
### 1. Conduct EDA to identify patterns and trends.
### 2. Visualize key variables and their relationships.
### 3. Analyze correlations between features and the target variable (likelihood of a video becoming viral).
### 4. Document key insights in a Jupyter notebook with visualizations.
### 5. Begin identifying potential features for the model based on EDA.

### Conduct EDA to identify patterns and trends.
"""

df.info()

df.describe(include='all')

df.head()

# We want to see the most common words that show up in description and tags
    # Keep in mind that videos may repeat
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
stop_words.update(['https', 'com', 'www', 'http'])

# Function to preprocess text: lowercase and remove special characters
def preprocess(text):
    if pd.isna(text):
        return []
    # Convert to lowercase and remove punctuation
    text = text.lower()
    # Remove any non-alphabetic characters and split words
    words = re.findall(r'\b\w+\b', text)
    words = [word for word in words if word not in stop_words]
    return words


df['tags_words'] = df['tags'].apply(preprocess)
df['description_words'] = df['description'].apply(preprocess)

all_tags_words = [word for words_list in df['tags_words'] for word in words_list]
all_description_words = [word for words_list in df['description_words'] for word in words_list]


tags_word_count = Counter(all_tags_words)
description_word_count = Counter(all_description_words)

common_tags = tags_word_count.most_common()
common_description = description_word_count.most_common()

tags_word_count_df = pd.DataFrame(common_tags, columns=['word', 'count'])
description_word_count_df = pd.DataFrame(common_description, columns=['word', 'count'])

print("Most common words in 'tags':")
print(tags_word_count_df.head(50))

print("\nMost common words in 'description':")
print(description_word_count_df.head(50))

!pip install wordcloud matplotlib

from wordcloud import WordCloud

# generate a word cloud
def generate_word_cloud(word_counts):
    word_dict = dict(word_counts)
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_dict)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()

print("Word Cloud for 'Tags':")
generate_word_cloud(common_tags)

print()
print()

print("Word Cloud for 'Description':")
generate_word_cloud(common_description)

# Distribution of view_count
plt.figure(figsize=(10, 6))
sns.histplot(df['view_count'], kde=True)
plt.title('Distribution of View Count')
plt.show()

# Distribution of likes
plt.figure(figsize=(10, 6))
sns.histplot(df['likes'], kde=True)
plt.title('Distribution of Likes')
plt.show()

# youtube removes dislikes end of 2021
# Distribution of dislikes
plt.figure(figsize=(10, 6))
sns.histplot(df['dislikes'], kde=True)
plt.title('Distribution of Dislikes')
plt.show()

df_without_outliers = df[~df.video_id.isin(z_anomalies_df.video_id)]
df_without_outliers.reset_index(inplace=True)
df_without_outliers

# Distribution of view_count
plt.figure(figsize=(10, 6))
sns.histplot(df_without_outliers['view_count'], kde=True)
plt.title('Distribution of View Count (No Outliers)')
plt.show()

# Distribution of likes
plt.figure(figsize=(10, 6))
sns.histplot(df_without_outliers['likes'], kde=True)
plt.title('Distribution of Likes (No Outliers)')
plt.show()

# Distribution of dislikes
plt.figure(figsize=(10, 6))
sns.histplot(df_without_outliers['dislikes'], kde=True)
plt.title('Distribution of Dislikes (No Outliers)')
plt.show()

# Scatter plot of view_count vs. likes
plt.figure(figsize=(10, 6))
sns.scatterplot(x='view_count', y='likes', data=df)
plt.title('View Count vs. Likes')
plt.show()

# Scatter plot of view_count vs. dislikes
plt.figure(figsize=(10, 6))
sns.scatterplot(x='view_count', y='dislikes', data=df)
plt.title('View Count vs. Dislikes')
plt.show()

# Scatter plot for view_count vs. net_likes with Logarithmic scale
df['net_likes'] = df['likes'] - df['dislikes']
plt.figure(figsize=(10, 6))

sns.scatterplot(x='view_count', y='net_likes', data=df, color='blue', label='Net Likes vs. View Count')

# set the x-axis to logarithmic scale
plt.xscale('log')
max_val = max(df['view_count'].max(), df['net_likes'].max())
plt.title('View Count vs. Net Likes (Log Scale)')
plt.xlabel('View Count (log scale)')
plt.ylabel('Net Likes (Likes - Dislikes)')
plt.legend()
plt.show()

# Scatter plot for view_count vs. net_likes excluding top 1% outliers

# filter out extreme outliers
threshold = np.percentile(df['view_count'], 99)  # top 1% are considered outliers
filtered_df = df[df['view_count'] < threshold]

plt.figure(figsize=(10, 6))
sns.scatterplot(x='view_count', y='net_likes', data=filtered_df, color='blue', label='Net Likes vs. View Count')
plt.plot([filtered_df['view_count'].min(), filtered_df['view_count'].max()], [filtered_df['view_count'].min(), filtered_df['view_count'].max()], linestyle='--', color='orange', label='y = x')
plt.title('View Count vs. Net Likes (Filtered)')
plt.xlabel('View Count')
plt.ylabel('Net Likes (Likes - Dislikes)')
plt.legend()
plt.show()

# Scatter plot of view_count vs. likes
plt.figure(figsize=(10, 6))
sns.scatterplot(x='view_count', y='likes', data=df_without_outliers)
plt.title('View Count vs. Likes (No Outliers)')
plt.show()

# Scatter plot of view_count vs. dislikes
plt.figure(figsize=(10, 6))
sns.scatterplot(x='view_count', y='dislikes', data=df_without_outliers)
plt.title('View Count vs. Dislikes (No Outliers)')
plt.show()

df.loc[df['view_count'].idxmax()]

# https://www.youtube.com/watch?v=cc2-4ci4G84
# ^ originally had 1.4 billion views because it was played each time discord was opened -> yt mass removed the views
# (between then-- april 1st 2024-- and now, it has only 3.9m views)

# Boxplot of view_count across categoryId
plt.figure(figsize=(12, 8))
sns.boxplot(x='category_name', y='view_count', data=df)
plt.title('View Count by Category')
plt.xticks(rotation=90)
plt.show()

# Boxplot of view_count across categoryId
plt.figure(figsize=(12, 8))
sns.boxplot(x='category_name', y='view_count', data=df_without_outliers)
plt.title('View Count by Category (No Outliers)')
plt.xticks(rotation=90)
plt.show()

# Bar plot of categoryId
plt.figure(figsize=(10, 6))
sns.countplot(y='category_name', data=df, order=df['category_name'].value_counts().index)
plt.title('Video Categories')
plt.show()

# Bar plot of comments_disabled
plt.figure(figsize=(10, 6))
sns.countplot(x='comments_disabled', data=df)
plt.title('Comments Disabled (0 or 1)')
plt.show()

# Bar plot of ratings_disabled
plt.figure(figsize=(10, 6))
sns.countplot(x='ratings_disabled', data=df)
plt.title('Ratings Disabled (0 or 1)')
plt.show()

print(df['publishedAt'].min())
print(df['publishedAt'].max())
# Extract date features
df['publish_year'] = df['publishedAt'].dt.year
df['publish_month'] = df['publishedAt'].dt.month

# Plot the number of videos published per year
plt.figure(figsize=(10, 6))
sns.countplot(x='publish_year', data=df)
plt.title('Number of Videos Published per Year')
plt.show()

# Plot the number of videos published per month
plt.figure(figsize=(10, 6))
sns.countplot(x='publish_month', data=df)
plt.title('Number of Videos Published per Month')
plt.show()

# Plot trends in view_count over time
plt.figure(figsize=(14, 8))
df.groupby('publish_year')['view_count'].sum().plot()
plt.title('Total View Count per Year')
plt.show()

# Correlation matrix
corr_matrix = df[numerical_cols_to_check].corr()

# Heatmap of the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

"""### Week 1 & 2 Recommended Deliverables:

1. Project Docs: Scope and deliverables
2. Jupyter Notebook: Data preprocessing steps, including code for cleaning and transforming the data
3. **TODO |** Markdown File: Summary of the data issues encountered and how they were addressed (to be submitted at the end of Week 2)

### Week 3 & 4 Recommended Deliverables:
1. Jupyter Notebook: EDA process, including code, visualizations, and insights
2. **TODO |** PowerPoint Slides: Key findings and potential features (to be presented internally at the end of Week 4)
"""

df.sort_values(by='view_count', ascending=False).head(40)

df['trending_date'] = pd.to_datetime(df['trending_date'])
df['publishedAt'] = pd.to_datetime(df['publishedAt'])
df['days_since_publication'] = (df['trending_date'] - df['publishedAt']).dt.days
df_sorted = df.sort_values(['video_id', 'days_since_publication'])
df_sorted['daily_growth_rate'] = df_sorted.groupby('video_id')['view_count'].pct_change() / df_sorted.groupby('video_id')['days_since_publication'].diff()
df_sorted['cumulative_growth_rate'] = (df_sorted['view_count'] / df_sorted.groupby('video_id')['view_count'].transform('first') - 1) / df_sorted['days_since_publication']

def plot_video_growth(video_id):
    video_data = df_sorted[df_sorted['video_id'] == video_id]

    plt.figure(figsize=(12, 6))
    plt.plot(video_data['days_since_publication'], video_data['daily_growth_rate'], label='Daily Growth Rate')
    plt.plot(video_data['days_since_publication'], video_data['cumulative_growth_rate'], label='Cumulative Growth Rate')

    plt.title(f"Growth Rates for Video: {video_id}")
    plt.xlabel("Days Since Publication")
    plt.ylabel("Growth Rate")
    plt.legend()
    plt.grid(True)
    plt.show()

for video_id in df_sorted['video_id'].unique():
    plot_video_growth(video_id)

avg_growth_rates = df_sorted.groupby('video_id').agg({
    'daily_growth_rate': 'mean',
    'cumulative_growth_rate': 'mean',
    'view_count': 'max',
    'title': 'first'
}).sort_values('cumulative_growth_rate', ascending=False)

print("Top 10 Videos by Cumulative Growth Rate:")
print(avg_growth_rates[['title', 'cumulative_growth_rate', 'view_count']].head(10))

plt.figure(figsize=(12, 6))
for video_id in df_sorted['video_id'].unique():
    video_data = df_sorted[df_sorted['video_id'] == video_id]
    plt.plot(video_data['days_since_publication'], video_data['cumulative_growth_rate'], label=video_id)

plt.title("Cumulative Growth Rates for All Videos")
plt.xlabel("Days Since Publication")
plt.ylabel("Cumulative Growth Rate")
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.tight_layout()
plt.show()

df['trending_date'] = pd.to_datetime(df['trending_date'])
df['publishedAt'] = pd.to_datetime(df['publishedAt'])
df['days_since_publication'] = (df['trending_date'] - df['publishedAt']).dt.days

df_sorted = df.sort_values(['video_id', 'days_since_publication'])
df_sorted['daily_growth_rate'] = df_sorted.groupby('video_id')['view_count'].pct_change() / df_sorted.groupby('video_id')['days_since_publication'].diff()
df_sorted['cumulative_growth_rate'] = (df_sorted['view_count'] / df_sorted.groupby('video_id')['view_count'].transform('first') - 1) / df_sorted['days_since_publication']
df_sorted['daily_growth_rate'] = df_sorted['daily_growth_rate'].replace([np.inf, -np.inf], np.nan)
df_sorted['cumulative_growth_rate'] = df_sorted['cumulative_growth_rate'].replace([np.inf, -np.inf], np.nan)

avg_growth_rates = df_sorted.groupby('video_id').agg({
    'daily_growth_rate': lambda x: x.mean(skipna=True),
    'cumulative_growth_rate': lambda x: x.mean(skipna=True),
    'view_count': 'max',
    'title': 'first',
    'category_name': 'first'
}).sort_values('cumulative_growth_rate', ascending=False)

avg_growth_rates = avg_growth_rates.dropna(subset=['daily_growth_rate', 'cumulative_growth_rate'])

print("Summary Statistics for Growth Rates:")
print(avg_growth_rates[['daily_growth_rate', 'cumulative_growth_rate']].describe())

print("\nTop 10 Videos by Cumulative Growth Rate:")
print(avg_growth_rates[['title', 'category_name', 'cumulative_growth_rate', 'view_count']].head(10))

plt.figure(figsize=(12, 6))
sns.histplot(avg_growth_rates['cumulative_growth_rate'], kde=True)
plt.title("Distribution of Cumulative Growth Rates")
plt.xlabel("Cumulative Growth Rate")
plt.ylabel("Count")
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(x='category_name', y='cumulative_growth_rate', data=avg_growth_rates)
plt.title("Cumulative Growth Rates by Category")
plt.xlabel("Category")
plt.ylabel("Cumulative Growth Rate")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))
plt.scatter(avg_growth_rates['view_count'], avg_growth_rates['cumulative_growth_rate'])
plt.title("View Count vs. Cumulative Growth Rate")
plt.xlabel("View Count")
plt.ylabel("Cumulative Growth Rate")
plt.xscale('log')
plt.show()

correlation = avg_growth_rates[['daily_growth_rate', 'cumulative_growth_rate', 'view_count']].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation, annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

df['trending_date'] = pd.to_datetime(df['trending_date'])
df['publishedAt'] = pd.to_datetime(df['publishedAt'])
df['days_since_publication'] = (df['trending_date'] - df['publishedAt']).dt.days

df_sorted = df.sort_values(['video_id', 'days_since_publication'])
df_sorted['daily_growth_rate'] = df_sorted.groupby('video_id')['view_count'].pct_change() / df_sorted.groupby('video_id')['days_since_publication'].diff()
df_sorted['cumulative_growth_rate'] = (df_sorted['view_count'] / df_sorted.groupby('video_id')['view_count'].transform('first') - 1) / df_sorted['days_since_publication']
df_sorted['daily_growth_rate'] = df_sorted['daily_growth_rate'].replace([np.inf, -np.inf], np.nan)
df_sorted['cumulative_growth_rate'] = df_sorted['cumulative_growth_rate'].replace([np.inf, -np.inf], np.nan)

avg_growth_rates = df_sorted.groupby('video_id').agg({
    'daily_growth_rate': lambda x: x.mean(skipna=True),
    'cumulative_growth_rate': lambda x: x.mean(skipna=True),
    'view_count': 'max',
    'title': 'first',
    'category_name': 'first'
}).sort_values('cumulative_growth_rate', ascending=False)

avg_growth_rates = avg_growth_rates.dropna(subset=['daily_growth_rate', 'cumulative_growth_rate'])

def plot_growth_rates_by_category(data, growth_rate_column, title):
    plt.figure(figsize=(15, 8))

    plt.subplot(1, 2, 1)
    sns.boxplot(x='category_name', y=growth_rate_column, data=data)
    plt.title(f"{title} - Box Plot")
    plt.xlabel("Category")
    plt.ylabel(growth_rate_column.replace('_', ' ').title())
    plt.xticks(rotation=45, ha='right')

    plt.subplot(1, 2, 2)
    sns.violinplot(x='category_name', y=growth_rate_column, data=data)
    plt.title(f"{title} - Violin Plot")
    plt.xlabel("Category")
    plt.ylabel(growth_rate_column.replace('_', ' ').title())
    plt.xticks(rotation=45, ha='right')

    plt.tight_layout()
    plt.show()

plot_growth_rates_by_category(avg_growth_rates, 'daily_growth_rate', "Daily Growth Rates by Category")
plot_growth_rates_by_category(avg_growth_rates, 'cumulative_growth_rate', "Cumulative Growth Rates by Category")

category_growth_rates = avg_growth_rates.groupby('category_name').agg({
    'daily_growth_rate': 'mean',
    'cumulative_growth_rate': 'mean'
}).sort_values('cumulative_growth_rate', ascending=False)

print("Average Growth Rates by Category:")
print(category_growth_rates)

plt.figure(figsize=(12, 6))
category_growth_rates['cumulative_growth_rate'].plot(kind='bar')
plt.title("Average Cumulative Growth Rate by Category")
plt.xlabel("Category")
plt.ylabel("Average Cumulative Growth Rate")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

df['trending_date'] = pd.to_datetime(df['trending_date'])
df['publishedAt'] = pd.to_datetime(df['publishedAt'])
df['days_since_publication'] = (df['trending_date'] - df['publishedAt']).dt.days
df_sorted = df.sort_values(['video_id', 'days_since_publication'])
df_sorted['cumulative_growth_rate'] = (df_sorted['view_count'] / df_sorted.groupby('video_id')['view_count'].transform('first') - 1) / df_sorted['days_since_publication'] # try alternative metrices
df_sorted['cumulative_growth_rate'] = df_sorted['cumulative_growth_rate'].replace([np.inf, -np.inf], np.nan)

df_sorted = df_sorted.dropna(subset=['cumulative_growth_rate'])
plt.figure(figsize=(16, 10))
scatter = plt.scatter(df_sorted['days_since_publication'],
                      df_sorted['cumulative_growth_rate'],
                      c=df_sorted['category_name'].astype('category').cat.codes,
                      alpha=0.5,
                      cmap='viridis')

plt.title('Video Growth Rate vs. Days Since Publication', fontsize=16)
plt.xlabel('Days Since Publication', fontsize=14)
plt.ylabel('Cumulative Growth Rate', fontsize=14)
plt.yscale('symlog')

categories = df_sorted['category_name'].unique()
handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.viridis(i / len(categories)),
                      markersize=8, alpha=0.5, label=cat) for i, cat in enumerate(categories)]
plt.legend(handles=handles, title='Category', loc='center left', bbox_to_anchor=(1, 0.5))

plt.tight_layout()
plt.show()

print("Summary Statistics:")
print(df_sorted.groupby('category_name').agg({
    'cumulative_growth_rate': ['mean', 'median', 'min', 'max'],
    'days_since_publication': ['mean', 'median', 'min', 'max']
}))

correlation = df_sorted['days_since_publication'].corr(df_sorted['cumulative_growth_rate'])
print(f"\nCorrelation between days since publication and cumulative growth rate: {correlation:.4f}")



"""# Part II

## Modeling

### Classification

####  Baseline Models
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.base import BaseEstimator, TransformerMixin

data=df
timestamp_columns = ['publishedAt', 'trending_date']
for col in timestamp_columns:
    data[col] = pd.to_datetime(data[col])
    data[f'{col}_year'] = data[col].dt.year
    data[f'{col}_month'] = data[col].dt.month
    data[f'{col}_day'] = data[col].dt.day

numeric_columns = data.select_dtypes(include=[np.number]).columns
X = data[numeric_columns].drop(['categoryId'], axis=1)
y = data['category_name']


X = X.fillna(X.mean()) # try other methods see which one works the best

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'SVM': SVC(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB(),
    'Neural Network': MLPClassifier(max_iter=1000)
}

results = {}

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy
    print(f"{name} Accuracy: {accuracy:.4f}")
    print(classification_report(y_test, y_pred))
    print("\n" + "="*50 + "\n")

print("Overall Results:")
for name, accuracy in results.items():
    print(f"{name}: {accuracy:.4f}")

"""#### Models with transformers and hyperparameter tuning"""

data=df

class FeatureEngineer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_ = X.copy()
        X_['publishedAt'] = pd.to_datetime(X_['publishedAt'])
        X_['trending_date'] = pd.to_datetime(X_['trending_date'])

        X_['days_to_trend'] = (X_['trending_date'] - X_['publishedAt']).dt.total_seconds() / 86400
        X_['title_length'] = X_['title'].str.len()
        X_['tags_count'] = X_['tags'].str.count('\|') + 1
        X_['description_length'] = X_['description'].str.len()

        X_['publishedAt_dayofweek'] = X_['publishedAt'].dt.dayofweek
        X_['publishedAt_hour'] = X_['publishedAt'].dt.hour

        return X_

features = ['days_to_trend', 'title_length', 'tags_count', 'description_length',
            'publishedAt_dayofweek', 'publishedAt_hour', 'categoryId']

virality_threshold = data['view_count'].quantile(0.9)

data['is_viral'] = (data['view_count'] > virality_threshold).astype(int)

X = data[['publishedAt', 'trending_date', 'title', 'tags', 'description', 'categoryId']]
y = data['is_viral']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

numeric_features = ['days_to_trend', 'title_length', 'tags_count', 'description_length',
                    'publishedAt_dayofweek', 'publishedAt_hour', 'categoryId']
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features)
    ])

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}

def evaluate_model(model, X_train, X_test, y_train, y_test):
    pipeline = Pipeline([
        ('feature_engineer', FeatureEngineer()),
        ('preprocessor', preprocessor),
        ('feature_selection', SelectKBest(f_classif, k='all')),
        ('classifier', model)
    ])

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, pipeline.predict_proba(X_test)[:, 1])

    return pipeline, accuracy, auc

results = {}

for name, model in models.items():
    pipeline, accuracy, auc = evaluate_model(model, X_train, X_test, y_train, y_test)
    results[name] = {'accuracy': accuracy, 'auc': auc, 'pipeline': pipeline}
    print(f"{name} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}")
    print(classification_report(y_test, pipeline.predict(X_test)))
    print("="*50)

best_model = max(results, key=lambda x: results[x]['auc'])
print(f"\nBest model: {best_model}")
print(f"Accuracy: {results[best_model]['accuracy']:.4f}")
print(f"AUC: {results[best_model]['auc']:.4f}")

best_pipeline = results[best_model]['pipeline']

param_grids = {
    'Logistic Regression': {
        'classifier__C': [0.1, 1, 10],
        'classifier__penalty': ['l1', 'l2'],
        'classifier__solver': ['liblinear', 'saga']
    },
    'Random Forest': {
        'classifier__n_estimators': [100, 200, 300],
        'classifier__max_depth': [3, 5, 7],
        'classifier__min_samples_split': [2, 5, 10]
    },
    'Gradient Boosting': {
        'classifier__n_estimators': [100, 200, 300],
        'classifier__max_depth': [3, 5, 7],
        'classifier__learning_rate': [0.01, 0.1, 0.3]
    }
}

param_grid = param_grids[best_model]

grid_search = GridSearchCV(best_pipeline, param_grid, cv=3, scoring='roc_auc')
grid_search.fit(X_train, y_train)

print("\nBest parameters:", grid_search.best_params_)
print("Best AUC:", grid_search.best_score_)

y_pred = grid_search.predict(X_test)
final_accuracy = accuracy_score(y_test, y_pred)
final_auc = roc_auc_score(y_test, grid_search.predict_proba(X_test)[:, 1])

print("\nFinal Model Performance:")
print(f"Accuracy: {final_accuracy:.4f}")
print(f"AUC: {final_auc:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

if hasattr(grid_search.best_estimator_.named_steps['classifier'], 'feature_importances_'):
    importances = grid_search.best_estimator_.named_steps['classifier'].feature_importances_
    feature_names = grid_search.best_estimator_.named_steps['feature_selection'].get_feature_names_out()
    feature_importance = pd.DataFrame({'feature': feature_names, 'importance': importances})
    print("\nFeature Importance:")
    print(feature_importance.sort_values('importance', ascending=False))
elif hasattr(grid_search.best_estimator_.named_steps['classifier'], 'coef_'):
    coefficients = grid_search.best_estimator_.named_steps['classifier'].coef_[0]
    feature_names = grid_search.best_estimator_.named_steps['feature_selection'].get_feature_names_out()
    feature_importance = pd.DataFrame({'feature': feature_names, 'coefficient': np.abs(coefficients)})
    print("\nFeature Importance (based on absolute coefficients):")
    print(feature_importance.sort_values('coefficient', ascending=False))

"""#### Visualization"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score
import numpy as np
import pandas as pd

# The best model and its pipeline are already defined
best_pipeline = results[best_model]['pipeline']

# Get predictions and probabilities
y_pred = best_pipeline.predict(X_test)
y_pred_proba = best_pipeline.predict_proba(X_test)[:, 1]

# AUC-ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")

# Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
average_precision = average_precision_score(y_test, y_pred_proba)

plt.subplot(1, 2, 2)
plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AP = {average_precision:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")

plt.tight_layout()
plt.show()

# Feature Importance Visualization
classifier = best_pipeline.named_steps['classifier']
if hasattr(classifier, 'feature_importances_'):
    importances = classifier.feature_importances_
    feature_names = best_pipeline.named_steps['feature_selection'].get_feature_names_out()
elif hasattr(classifier, 'coef_'):
    importances = np.abs(classifier.coef_[0])
    feature_names = best_pipeline.named_steps['feature_selection'].get_feature_names_out()
else:
    print("Feature importance not available for this model.")
    importances = []
    feature_names = []

if len(importances) > 0:
    feature_importance = pd.DataFrame({'feature': feature_names, 'importance': importances})
    feature_importance = feature_importance.sort_values('importance', ascending=False)

    plt.figure(figsize=(10, 6))
    plt.bar(feature_importance['feature'], feature_importance['importance'])
    plt.xticks(rotation=45, ha='right')
    plt.xlabel('Features')
    plt.ylabel('Importance')
    plt.title(f'Feature Importance for {best_model}')
    plt.tight_layout()
    plt.show()

print(f"Best model: {best_model}")
print(f"AUC: {roc_auc:.4f}")
print(f"Average Precision: {average_precision:.4f}")

"""### Regression

#### Linear Regression, Huber Regression, Random Forest, Gradient Boosting
"""

data=df

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, HuberRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.base import BaseEstimator, TransformerMixin
import matplotlib.pyplot as plt
import seaborn as sns


class FeatureEngineer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_ = X.copy()
        X_['publishedAt'] = pd.to_datetime(X_['publishedAt'])
        X_['trending_date'] = pd.to_datetime(X_['trending_date'])

        X_['days_since_published'] = (X_['trending_date'] - X_['publishedAt']).dt.total_seconds() / 86400
        X_['title_length'] = X_['title'].str.len()
        X_['tags_count'] = X_['tags'].str.count('\|') + 1
        X_['description_length'] = X_['description'].str.len()

        X_['publishedAt_dayofweek'] = X_['publishedAt'].dt.dayofweek
        X_['publishedAt_hour'] = X_['publishedAt'].dt.hour
        X_['publishedAt_year'] = X_['publishedAt'].dt.year
        X_['publishedAt_month'] = X_['publishedAt'].dt.month

        X_['trendingDate_year'] = X_['trending_date'].dt.year
        X_['trendingDate_month'] = X_['trending_date'].dt.month

        X_['days_tags_interaction'] = X_['days_since_published'] * X_['tags_count']

        X_['likes_growth_rate'] = X_['likes'] / (X_['days_since_published'] + 1e-6)
        X_['comment_count_growth_rate'] = X_['comment_count'] / (X_['days_since_published'] + 1e-6)

        return X_


features = ['publishedAt', 'trending_date', 'title', 'tags', 'description', 'categoryId',
           'comment_count', 'likes']
target = 'view_count'

plt.figure(figsize=(10, 6))
sns.histplot(data[target], kde=True)
plt.title('Distribution of View Count')
plt.xlabel('View Count')
plt.show()

data['log_view_count'] = np.log1p(data[target])

X = data[features]
y = data['log_view_count']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

numeric_features = ['days_since_published', 'comment_count', 'likes', 'title_length', 'tags_count',
                   'description_length', 'publishedAt_dayofweek', 'publishedAt_hour', 'categoryId',
                   'days_tags_interaction', 'likes_growth_rate', 'comment_count_growth_rate',
                   'publishedAt_year', 'publishedAt_month', 'trendingDate_year', 'trendingDate_month']

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('poly', PolynomialFeatures(degree=2, include_bias=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features)
    ])

models = {
    'Linear Regression': LinearRegression(),
    'Huber Regression': HuberRegressor(),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

def evaluate_model(model, X_train, X_test, y_train, y_test):
    pipeline = Pipeline([
        ('feature_engineer', FeatureEngineer()),
        ('preprocessor', preprocessor),
        ('feature_selection', SelectKBest(f_regression, k='all')),
        ('regressor', model)
    ])

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    return pipeline, mse, r2

results = {}

for name, model in models.items():
    pipeline, mse, r2 = evaluate_model(model, X_train, X_test, y_train, y_test)
    results[name] = {'mse': mse, 'r2': r2, 'pipeline': pipeline}
    print(f"{name} - MSE: {mse:.4f}, R2: {r2:.4f}")
    print("="*50)

best_model = max(results, key=lambda x: results[x]['r2'])
print(f"\nBest model: {best_model}")
print(f"MSE: {results[best_model]['mse']:.4f}")
print(f"R2: {results[best_model]['r2']:.4f}")

best_pipeline = results[best_model]['pipeline']


# GridSearchCV
param_grids = {
    'Linear Regression': {},
    'Huber Regression': {
        'regressor__epsilon': [1.1, 1.35, 1.5],
        'regressor__alpha': [0.0001, 0.001, 0.01]
    },
    'Random Forest': {
        'regressor__n_estimators': [100, 200, 300],
        'regressor__max_depth': [None, 10, 20],
        'regressor__min_samples_split': [2, 5, 10]
    },
    'Gradient Boosting': {
        'regressor__n_estimators': [100, 200, 300],
        'regressor__max_depth': [3, 5, 7],
        'regressor__learning_rate': [0.01, 0.1, 0.3]
    }
}

# GridSearchCV on best model
param_grid = param_grids[best_model]

if param_grid:
    grid_search = GridSearchCV(best_pipeline, param_grid, cv=3, scoring='neg_mean_squared_error')
    grid_search.fit(X_train, y_train)

    print("\nBest parameters:", grid_search.best_params_)
    print("Best MSE:", -grid_search.best_score_)

    y_pred = grid_search.predict(X_test)
else:
    y_pred = best_pipeline.predict(X_test)

y_pred_original = np.expm1(y_pred)
y_test_original = np.expm1(y_test)

final_mse = mean_squared_error(y_test_original, y_pred_original)
final_r2 = r2_score(y_test_original, y_pred_original)

print("\nFinal Model Performance (on original scale):")
print(f"MSE: {final_mse:.2f}")
print(f"R2: {final_r2:.4f}")

if hasattr(best_pipeline.named_steps['regressor'], 'feature_importances_'):
    importances = best_pipeline.named_steps['regressor'].feature_importances_
    feature_names = best_pipeline.named_steps['feature_selection'].get_feature_names_out()
    feature_importance = pd.DataFrame({'feature': feature_names, 'importance': importances})
    print("\nFeature Importance:")
    print(feature_importance.sort_values('importance', ascending=False))
elif hasattr(best_pipeline.named_steps['regressor'], 'coef_'):
    coefficients = best_pipeline.named_steps['regressor'].coef_
    feature_names = best_pipeline.named_steps['feature_selection'].get_feature_names_out()
    feature_importance = pd.DataFrame({'feature': feature_names, 'coefficient': np.abs(coefficients)})
    print("\nFeature Importance (based on absolute coefficients):")
    print(feature_importance.sort_values('coefficient', ascending=False))

plt.figure(figsize=(10, 6))
plt.scatter(y_test_original, y_pred_original, alpha=0.5)
plt.plot([y_test_original.min(), y_test_original.max()], [y_test_original.min(), y_test_original.max()], 'r--', lw=2)
plt.xlabel('Actual View Count')
plt.ylabel('Predicted View Count')
plt.title('Predicted vs Actual View Count')
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, HuberRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.base import BaseEstimator, TransformerMixin
import matplotlib.pyplot as plt
import seaborn as sns


class FeatureEngineer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_ = X.copy()
        X_['publishedAt'] = pd.to_datetime(X_['publishedAt'])
        X_['trending_date'] = pd.to_datetime(X_['trending_date'])

        X_['days_since_published'] = (X_['trending_date'] - X_['publishedAt']).dt.total_seconds() / 86400
        X_['title_length'] = X_['title'].str.len()
        X_['tags_count'] = X_['tags'].str.count('\|') + 1
        X_['description_length'] = X_['description'].str.len()

        X_['publishedAt_dayofweek'] = X_['publishedAt'].dt.dayofweek
        X_['publishedAt_hour'] = X_['publishedAt'].dt.hour
        X_['publishedAt_year'] = X_['publishedAt'].dt.year
        X_['publishedAt_month'] = X_['publishedAt'].dt.month

        X_['trendingDate_year'] = X_['trending_date'].dt.year
        X_['trendingDate_month'] = X_['trending_date'].dt.month

        X_['days_tags_interaction'] = X_['days_since_published'] * X_['tags_count']

        X_['likes_growth_rate'] = X_['likes'] / (X_['days_since_published'] + 1e-6)
        X_['comment_count_growth_rate'] = X_['comment_count'] / (X_['days_since_published'] + 1e-6)

        return X_


features = ['publishedAt', 'trending_date', 'title', 'tags', 'description', 'categoryId',
           'comment_count', 'likes']
target = 'view_count'

plt.figure(figsize=(10, 6))
sns.histplot(data[target], kde=True)
plt.title('Distribution of View Count')
plt.xlabel('View Count')
plt.show()

data['log_view_count'] = np.log1p(data[target])

X = data[features]
y = data['log_view_count']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

numeric_features = ['days_since_published', 'comment_count', 'likes', 'title_length', 'tags_count',
                   'description_length', 'publishedAt_dayofweek', 'publishedAt_hour', 'categoryId',
                   'days_tags_interaction', 'likes_growth_rate', 'comment_count_growth_rate',
                   'publishedAt_year', 'publishedAt_month', 'trendingDate_year', 'trendingDate_month']

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('poly', PolynomialFeatures(degree=2, include_bias=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features)
    ])

models = {
    'Linear Regression': LinearRegression(),
    'Huber Regression': HuberRegressor(),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

def evaluate_model(model, X_train, X_test, y_train, y_test):
    pipeline = Pipeline([
        ('feature_engineer', FeatureEngineer()),
        ('preprocessor', preprocessor),
        ('feature_selection', SelectKBest(f_regression, k='all')),
        ('regressor', model)
    ])

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    return pipeline, mse, r2

results = {}

for name, model in models.items():
    pipeline, mse, r2 = evaluate_model(model, X_train, X_test, y_train, y_test)
    results[name] = {'mse': mse, 'r2': r2, 'pipeline': pipeline}
    print(f"{name} - MSE: {mse:.4f}, R2: {r2:.4f}")
    print("="*50)

best_model = max(results, key=lambda x: results[x]['r2'])
print(f"\nBest model: {best_model}")
print(f"MSE: {results[best_model]['mse']:.4f}")
print(f"R2: {results[best_model]['r2']:.4f}")

best_pipeline = results[best_model]['pipeline']


# GridSearchCV
param_grids = {
    'Linear Regression': {},
    'Huber Regression': {
        'regressor__epsilon': [1.1, 1.35, 1.5],
        'regressor__alpha': [0.0001, 0.001, 0.01]
    },
    'Random Forest': {
        'regressor__n_estimators': [100, 200, 300],
        'regressor__max_depth': [None, 10, 20],
        'regressor__min_samples_split': [2, 5, 10]
    },
    'Gradient Boosting': {
        'regressor__n_estimators': [100, 200, 300],
        'regressor__max_depth': [3, 5, 7],
        'regressor__learning_rate': [0.01, 0.1, 0.3]
    }
}

# GridSearchCV on best model
param_grid = param_grids[best_model]

if param_grid:
    grid_search = GridSearchCV(best_pipeline, param_grid, cv=3, scoring='neg_mean_squared_error')
    grid_search.fit(X_train, y_train)

    print("\nBest parameters:", grid_search.best_params_)
    print("Best MSE:", -grid_search.best_score_)

    y_pred = grid_search.predict(X_test)
else:
    y_pred = best_pipeline.predict(X_test)

y_pred_original = np.expm1(y_pred)
y_test_original = np.expm1(y_test)

final_mse = mean_squared_error(y_test_original, y_pred_original)
final_r2 = r2_score(y_test_original, y_pred_original)

print("\nFinal Model Performance (on original scale):")
print(f"MSE: {final_mse:.2f}")
print(f"R2: {final_r2:.4f}")

if hasattr(best_pipeline.named_steps['regressor'], 'feature_importances_'):
    importances = best_pipeline.named_steps['regressor'].feature_importances_
    feature_names = best_pipeline.named_steps['feature_selection'].get_feature_names_out()
    feature_importance = pd.DataFrame({'feature': feature_names, 'importance': importances})
    print("\nFeature Importance:")
    print(feature_importance.sort_values('importance', ascending=False))
elif hasattr(best_pipeline.named_steps['regressor'], 'coef_'):
    coefficients = best_pipeline.named_steps['regressor'].coef_
    feature_names = best_pipeline.named_steps['feature_selection'].get_feature_names_out()
    feature_importance = pd.DataFrame({'feature': feature_names, 'coefficient': np.abs(coefficients)})
    print("\nFeature Importance (based on absolute coefficients):")
    print(feature_importance.sort_values('coefficient', ascending=False))

plt.figure(figsize=(10, 6))
plt.scatter(y_test_original, y_pred_original, alpha=0.5)
plt.plot([y_test_original.min(), y_test_original.max()], [y_test_original.min(), y_test_original.max()], 'r--', lw=2)
plt.xlabel('Actual View Count')
plt.ylabel('Predicted View Count')
plt.title('Predicted vs Actual View Count')
plt.show()

data = df

pip install optuna

"""#### Neural Networks

##### First Trail
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau


features = ['publishedAt', 'trending_date', 'title', 'tags', 'description', 'categoryId',
           'comment_count', 'likes']
target = 'view_count'

X = data[features]
y = np.log1p(data[target])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

fe = FeatureEngineer()
X_train_fe = fe.fit_transform(X_train)
X_test_fe = fe.transform(X_test)


numeric_features = ['days_since_published', 'comment_count', 'likes', 'title_length', 'tags_count',
                   'description_length', 'publishedAt_dayofweek', 'publishedAt_hour', 'categoryId',
                   'days_tags_interaction', 'likes_growth_rate', 'comment_count_growth_rate',
                   'publishedAt_year', 'publishedAt_month', 'trendingDate_year', 'trendingDate_month']

categorical_features = ['categoryId']
text_features = ['title', 'tags', 'description']

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value=-1)),
    ('to_string', FunctionTransformer(lambda x: x.astype(str))),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

X_train_processed = preprocessor.fit_transform(X_train_fe)
X_test_processed = preprocessor.transform(X_test_fe)

max_words = 512
max_len = 100

tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train_fe['title'] + ' ' + X_train_fe['tags'] ) # deleted description, too much noise --  code + ' ' + X_train_fe['description']

X_train_text = tokenizer.texts_to_sequences(X_train_fe['title'] + ' ' + X_train_fe['tags']) #  + ' ' + X_train_fe['description']
X_test_text = tokenizer.texts_to_sequences(X_test_fe['title'] + ' ' + X_test_fe['tags']) # + ' ' + X_test_fe['description']

X_train_text = tf.keras.preprocessing.sequence.pad_sequences(X_train_text, maxlen=max_len)
X_test_text = tf.keras.preprocessing.sequence.pad_sequences(X_test_text, maxlen=max_len)

def create_model(input_dim, text_input_dim, embed_dim=128, num_heads=4):
    input_features = Input(shape=(input_dim,))
    x = Dense(256, activation='relu')(input_features)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)

    text_input = Input(shape=(max_len,))
    embedding = Embedding(max_words, embed_dim)(text_input)

    attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(embedding, embedding)
    attention = Flatten()(attention)

    combined = Concatenate()([x, attention])

    x = Dense(512, activation='relu')(combined)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    x = Dense(256, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)

    output = Dense(1)(x)

    model = Model(inputs=[input_features, text_input], outputs=output)
    return model

input_dim = X_train_processed.shape[1]
model = create_model(input_dim, max_len)
model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')

early_stopping = EarlyStopping(patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(factor=0.2, patience=5, min_lr=1e-6)

history = model.fit(
    [X_train_processed, X_train_text], y_train,
    validation_split=0.2,
    epochs=5,
    batch_size=64,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

y_pred = model.predict([X_test_processed, X_test_text])
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.4f}")
print(f"R2: {r2:.4f}")

y_pred_original = np.expm1(y_pred)
y_test_original = np.expm1(y_test)

final_mse = mean_squared_error(y_test_original, y_pred_original)
final_r2 = r2_score(y_test_original, y_pred_original)

print("\nFinal Model Performance (on original scale):")
print(f"MSE: {final_mse:.2f}")
print(f"R2: {final_r2:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.scatter(y_test_original, y_pred_original, alpha=0.5)
plt.plot([y_test_original.min(), y_test_original.max()], [y_test_original.min(), y_test_original.max()], 'r--', lw=2)
plt.xlabel('Actual View Count')
plt.ylabel('Predicted View Count')
plt.title('Predicted vs Actual View Count')
plt.tight_layout()
plt.show()

"""**Observations and To-Dos:** The model might be starting to overfit the training data, especially for epoch 3,4, and 5 we can see that the validation loss is increasing while the training loss is decreasing. There could be issues with the validation data, like outliers or mislabels, that are causing the validation loss to spike. Or, the learning rate might be too high, causing the model to overshoot optimal weights and causing large fluctuations in the loss.

##### Second Trial
"""

data=df

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Dense, Embedding, LSTM, Bidirectional,
    Dropout, BatchNormalization, Concatenate, GlobalAveragePooling1D,
    LayerNormalization, MultiHeadAttention, Add
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l1_l2
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
import datetime

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Dense, Embedding, LSTM, Bidirectional,
    Dropout, BatchNormalization, Concatenate
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt

FEATURES = [
    'title', 'publishedAt', 'categoryId', 'trending_date', 'tags',
    'likes', 'comment_count', 'description',
    'publishedAt_year', 'publishedAt_month', 'publishedAt_date', 'publishedAt_time',
    'trendingDate_year', 'trendingDate_month', 'trendingDate_date'
]

class AdvancedFeatureEngineer:
    def __init__(self):
        self.le = LabelEncoder()

    def safe_numeric_convert(self, series):
        """Safely convert series to numeric values"""
        if isinstance(series, (pd.Series, np.ndarray)):
            return pd.to_numeric(series, errors='coerce').fillna(0)
        return series

    def extract_hour(self, time_val):
        """Safely extract hour from various time formats"""
        if pd.isna(time_val):
            return -1
        try:
            if isinstance(time_val, str):
                return pd.to_datetime(time_val).hour
            elif isinstance(time_val, (datetime.time, pd.Timestamp)):
                return time_val.hour
            return -1
        except:
            return -1

    def fit_transform(self, X):
        X_ = X.copy()

        X_['publishedAt_dt'] = pd.to_datetime(X_['publishedAt'])
        X_['trending_dt'] = pd.to_datetime(X_['trending_date'])

        X_['days_between'] = (X_['trending_dt'] - X_['publishedAt_dt']).dt.total_seconds() / 86400
        X_['publish_hour'] = X_['publishedAt_time'].apply(self.extract_hour)

        X_['title_length'] = X_['title'].astype(str).str.len()
        X_['tags_count'] = X_['tags'].astype(str).str.count('\|') + 1
        X_['description_length'] = X_['description'].astype(str).str.len()

        X_['title_word_count'] = X_['title'].astype(str).str.split().str.len()
        X_['description_word_count'] = X_['description'].astype(str).str.split().str.len()

        numeric_cols = ['likes', 'comment_count']
        for col in numeric_cols:
            X_[col] = self.safe_numeric_convert(X_[col])

        X_['likes_per_day'] = X_['likes'] / (X_['days_between'] + 1)
        X_['comments_per_day'] = X_['comment_count'] / (X_['days_between'] + 1)
        X_['engagement_ratio'] = (X_['likes'] + X_['comment_count']) / (X_['days_between'] + 1)
        X_['engagement_per_word'] = X_['engagement_ratio'] / (X_['title_word_count'] + 1)
        X_['likes_per_tag'] = X_['likes'] / (X_['tags_count'] + 1)
        X_['comments_per_tag'] = X_['comment_count'] / (X_['tags_count'] + 1)

        X_['categoryId'] = self.le.fit_transform(X_['categoryId'].astype(str))

        X_['publish_is_weekend'] = X_['publishedAt_dt'].dt.dayofweek.isin([5, 6]).astype(int)
        X_['trending_is_weekend'] = pd.to_datetime(X_['trending_date']).dt.dayofweek.isin([5, 6]).astype(int)

        final_features = [
            'categoryId', 'likes', 'comment_count',
            'publishedAt_year', 'publishedAt_month',
            'trendingDate_year', 'trendingDate_month',
            'days_between', 'title_length', 'tags_count',
            'description_length', 'likes_per_day', 'comments_per_day',
            'engagement_ratio', 'engagement_per_word', 'likes_per_tag',
            'comments_per_tag', 'publish_hour', 'publish_is_weekend',
            'trending_is_weekend', 'title_word_count', 'description_word_count'
        ]

        for col in final_features:
            if col not in X_.columns:
                X_[col] = 0
            else:
                X_[col] = self.safe_numeric_convert(X_[col])

        return X_

    def transform(self, X):
        return self.fit_transform(X)

def prepare_text_data(texts, tokenizer=None, max_words=10000, max_len=50, fit=False):
    processed_texts = [str(text) if pd.notnull(text) else '' for text in texts]

    if fit:
        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)
        tokenizer.fit_on_texts(processed_texts)

    sequences = tokenizer.texts_to_sequences(processed_texts)
    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)

    return (padded_sequences, tokenizer) if fit else padded_sequences

def create_efficient_model(input_dim, text_vocab_size, max_len=50):
    num_input = Input(shape=(input_dim,))
    x_num = Dense(256, activation='relu')(num_input)
    x_num = BatchNormalization()(x_num)
    x_num = Dropout(0.3)(x_num)

    text_input = Input(shape=(max_len,))
    x_text = Embedding(text_vocab_size, 128)(text_input)
    x_text = Bidirectional(LSTM(64))(x_text)
    x_text = Dropout(0.3)(x_text)

    x = Concatenate()([x_num, x_text])

    x = Dense(256, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)

    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)

    output = Dense(1)(x)

    return Model(inputs=[num_input, text_input], outputs=output)

def train_model(X_train, X_test, y_train, y_test):
    print("Initial shapes:")
    print(f"X_train: {X_train.shape}")
    print(f"y_train: {y_train.shape}")

    fe = AdvancedFeatureEngineer()
    X_train_fe = fe.fit_transform(X_train)
    X_test_fe = fe.transform(X_test)

    numeric_features = [col for col in X_train_fe.columns if col not in
                       ['title', 'tags', 'description', 'publishedAt', 'trending_date',
                        'publishedAt_dt', 'trending_dt', 'publishedAt_time']]

    train_texts = [f"{title} {tags}" for title, tags
                  in zip(X_train_fe['title'].fillna(''), X_train_fe['tags'].fillna(''))]
    test_texts = [f"{title} {tags}" for title, tags
                 in zip(X_test_fe['title'].fillna(''), X_test_fe['tags'].fillna(''))]

    X_train_text, tokenizer = prepare_text_data(train_texts, fit=True)
    X_test_text = prepare_text_data(test_texts, tokenizer=tokenizer)
    scaler = StandardScaler()
    X_train_num = scaler.fit_transform(X_train_fe[numeric_features])
    X_test_num = scaler.transform(X_test_fe[numeric_features])

    print("\nProcessed shapes:")
    print(f"X_train_num: {X_train_num.shape}")
    print(f"X_train_text: {X_train_text.shape}")

    model = create_efficient_model(
        input_dim=len(numeric_features),
        text_vocab_size=10000,
        max_len=50
    )

    model.compile(
        optimizer=Adam(learning_rate=1e-3),
        loss='mse'
    )

    history = model.fit(
        [X_train_num, X_train_text],
        y_train,
        validation_split=0.2,
        epochs=20,
        batch_size=256,
        callbacks=[
            EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)
        ],
        verbose=1
    )

    y_pred = model.predict([X_test_num, X_test_text], batch_size=256)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    return model, history, mse, r2, y_pred

def evaluate_and_visualize(y_test, y_pred, y_test_original, y_pred_original, history):
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 3, 2)
    plt.scatter(y_test, y_pred, alpha=0.5, s=20)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('Actual View Count (log scale)')
    plt.ylabel('Predicted View Count (log scale)')
    plt.title(f'Log Scale Predictions\nR² = {r2_score(y_test, y_pred):.4f}')

    plt.subplot(1, 3, 3)
    y_test_m = y_test_original / 1e6
    y_pred_m = y_pred_original / 1e6
    plt.scatter(y_test_m, y_pred_m, alpha=0.5, s=20)
    plt.plot([y_test_m.min(), y_test_m.max()], [y_test_m.min(), y_test_m.max()], 'r--', lw=2)
    plt.xlabel('Actual Views (millions)')
    plt.ylabel('Predicted Views (millions)')
    plt.title('Original Scale Predictions')

    plt.tight_layout()
    plt.show()

    print("\nPerformance Metrics:")
    print(f"Log Scale R²: {r2_score(y_test, y_pred):.4f}")

    ranges = [
        (0, 1e6, '<1M'),
        (1e6, 10e6, '1M-10M'),
        (10e6, 100e6, '10M-100M'),
        (100e6, float('inf'), '>100M')
    ]

if __name__ == "__main__":
    X = data[FEATURES]
    y = np.log1p(data['view_count'])
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model, history, mse, r2, y_pred = train_model(X_train, X_test, y_train, y_test)

    y_pred_original = np.expm1(y_pred)
    y_test_original = np.expm1(y_test)

    evaluate_and_visualize(y_test, y_pred, y_test_original, y_pred_original, history)

model.summary()

from IPython.display import SVG
SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))

"""##### Final Model"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Dense, Embedding, LSTM, Bidirectional,
    Dropout, BatchNormalization, Concatenate
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
import keras_tuner as kt
from tensorflow.keras import backend as K

FEATURES = [
    'title', 'publishedAt', 'categoryId', 'trending_date', 'tags',
    'likes', 'comment_count', 'description',
    'publishedAt_year', 'publishedAt_month', 'publishedAt_date', 'publishedAt_time',
    'trendingDate_year', 'trendingDate_month', 'trendingDate_date'
]

class AdvancedFeatureEngineer:
    def __init__(self):
        self.le = LabelEncoder()

    def safe_numeric_convert(self, series):
        """Safely convert series to numeric values"""
        if isinstance(series, (pd.Series, np.ndarray)):
            return pd.to_numeric(series, errors='coerce').fillna(0)
        return series

    def extract_hour(self, time_val):
        """Safely extract hour from various time formats"""
        if pd.isna(time_val):
            return -1
        try:
            if isinstance(time_val, str):
                return pd.to_datetime(time_val).hour
            elif isinstance(time_val, (datetime.time, pd.Timestamp)):
                return time_val.hour
            return -1
        except:
            return -1

    def fit_transform(self, X):
        X_ = X.copy()

        X_['publishedAt_dt'] = pd.to_datetime(X_['publishedAt'])
        X_['trending_dt'] = pd.to_datetime(X_['trending_date'])

        X_['days_between'] = (X_['trending_dt'] - X_['publishedAt_dt']).dt.total_seconds() / 86400
        X_['publish_hour'] = X_['publishedAt_time'].apply(self.extract_hour)

        X_['title_length'] = X_['title'].astype(str).str.len()
        X_['tags_count'] = X_['tags'].astype(str).str.count('\|') + 1
        X_['description_length'] = X_['description'].astype(str).str.len()

        X_['title_word_count'] = X_['title'].astype(str).str.split().str.len()
        X_['description_word_count'] = X_['description'].astype(str).str.split().str.len()

        numeric_cols = ['likes', 'comment_count']
        for col in numeric_cols:
            X_[col] = self.safe_numeric_convert(X_[col])

        X_['likes_per_day'] = X_['likes'] / (X_['days_between'] + 1)
        X_['comments_per_day'] = X_['comment_count'] / (X_['days_between'] + 1)
        X_['engagement_ratio'] = (X_['likes'] + X_['comment_count']) / (X_['days_between'] + 1)
        X_['engagement_per_word'] = X_['engagement_ratio'] / (X_['title_word_count'] + 1)
        X_['likes_per_tag'] = X_['likes'] / (X_['tags_count'] + 1)
        X_['comments_per_tag'] = X_['comment_count'] / (X_['tags_count'] + 1)

        X_['categoryId'] = self.le.fit_transform(X_['categoryId'].astype(str))

        X_['publish_is_weekend'] = X_['publishedAt_dt'].dt.dayofweek.isin([5, 6]).astype(int)
        X_['trending_is_weekend'] = pd.to_datetime(X_['trending_date']).dt.dayofweek.isin([5, 6]).astype(int)

        final_features = [
            'categoryId', 'likes', 'comment_count',
            'publishedAt_year', 'publishedAt_month',
            'trendingDate_year', 'trendingDate_month',
            'days_between', 'title_length', 'tags_count',
            'description_length', 'likes_per_day', 'comments_per_day',
            'engagement_ratio', 'engagement_per_word', 'likes_per_tag',
            'comments_per_tag', 'publish_hour', 'publish_is_weekend',
            'trending_is_weekend', 'title_word_count', 'description_word_count'
        ]

        for col in final_features:
            if col not in X_.columns:
                X_[col] = 0
            else:
                X_[col] = self.safe_numeric_convert(X_[col])

        return X_

    def transform(self, X):
        return self.fit_transform(X)

def prepare_text_data(texts, tokenizer=None, max_words=10000, max_len=50, fit=False):
    processed_texts = [str(text) if pd.notnull(text) else '' for text in texts]

    if fit:
        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)
        tokenizer.fit_on_texts(processed_texts)

    sequences = tokenizer.texts_to_sequences(processed_texts)
    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)

    return (padded_sequences, tokenizer) if fit else padded_sequences

def create_tunable_model(hp, num_features):
    num_input = Input(shape=(num_features,))

    x_num = Dense(
        units=hp.Int('num_dense_units', min_value=128, max_value=512, step=64),
        activation='relu'
    )(num_input)
    x_num = BatchNormalization()(x_num)
    x_num = Dropout(hp.Float('num_dropout', min_value=0.1, max_value=0.5, step=0.1))(x_num)

    text_input = Input(shape=(50,))

    embedding_dim = hp.Int('embedding_dim', min_value=64, max_value=256, step=32)
    x_text = Embedding(10000, embedding_dim)(text_input)

    lstm_units = hp.Int('lstm_units', min_value=32, max_value=128, step=32)
    x_text = Bidirectional(LSTM(lstm_units))(x_text)
    x_text = Dropout(hp.Float('text_dropout', min_value=0.1, max_value=0.5, step=0.1))(x_text)

    x = Concatenate()([x_num, x_text])

    for i in range(hp.Int('num_dense_layers', 1, 3)):
        x = Dense(
            units=hp.Int(f'dense_{i}_units', min_value=64, max_value=256, step=32),
            activation='relu'
        )(x)
        x = BatchNormalization()(x)
        x = Dropout(hp.Float(f'dense_{i}_dropout', min_value=0.1, max_value=0.4, step=0.1))(x)

    output = Dense(1)(x)

    model = Model(inputs=[num_input, text_input], outputs=output)

    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss='mse'
    )

    return model

def train_model_with_tuning(X_train, X_test, y_train, y_test):
    print("Initial shapes:")
    print(f"X_train: {X_train.shape}")
    print(f"y_train: {y_train.shape}")

    fe = AdvancedFeatureEngineer()
    X_train_fe = fe.fit_transform(X_train)
    X_test_fe = fe.transform(X_test)

    numeric_features = [col for col in X_train_fe.columns if col not in
                       ['title', 'tags', 'description', 'publishedAt', 'trending_date',
                        'publishedAt_dt', 'trending_dt', 'publishedAt_time']]

    print(f"Number of numeric features: {len(numeric_features)}")

    train_texts = [f"{title} {tags}" for title, tags
                  in zip(X_train_fe['title'].fillna(''), X_train_fe['tags'].fillna(''))]
    test_texts = [f"{title} {tags}" for title, tags
                 in zip(X_test_fe['title'].fillna(''), X_test_fe['tags'].fillna(''))]

    X_train_text, tokenizer = prepare_text_data(train_texts, fit=True)
    X_test_text = prepare_text_data(test_texts, tokenizer=tokenizer)

    scaler = StandardScaler()
    X_train_num = scaler.fit_transform(X_train_fe[numeric_features])
    X_test_num = scaler.transform(X_test_fe[numeric_features])

    print("\nProcessed shapes:")
    print(f"X_train_num shape: {X_train_num.shape}")
    print(f"X_train_text shape: {X_train_text.shape}")

    def build_model(hp):
        return create_tunable_model(hp, num_features=X_train_num.shape[1])

    tuner = kt.Hyperband(
        build_model,
        objective='val_loss',
        max_epochs=20,
        factor=3,
        directory='hyperband_tuning',
        project_name='youtube_views_prediction',
        overwrite=True
    )

    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True
    )

    tuner.search(
        [X_train_num, X_train_text],
        y_train,
        validation_split=0.2,
        callbacks=[early_stopping],
        batch_size=256
    )

    best_hps = tuner.get_best_hyperparameters(1)[0]
    print("\nBest Hyperparameters:")
    for param, value in best_hps.values.items():
        print(f"{param}: {value}")

    model = tuner.hypermodel.build(best_hps)

    history = model.fit(
        [X_train_num, X_train_text],
        y_train,
        validation_split=0.2,
        epochs=20,
        batch_size=256,
        callbacks=[
            early_stopping,
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)
        ],
        verbose=1
    )

    y_pred = model.predict([X_test_num, X_test_text], batch_size=256)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    return model, history, mse, r2, y_pred, best_hps

def evaluate_and_visualize(y_test, y_pred, y_test_original, y_pred_original, history):
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 3, 2)
    plt.scatter(y_test, y_pred, alpha=0.5, s=20)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('Actual View Count (log scale)')
    plt.ylabel('Predicted View Count (log scale)')
    plt.title(f'Log Scale Predictions\nR² = {r2_score(y_test, y_pred):.4f}')

    plt.subplot(1, 3, 3)
    y_test_m = y_test_original / 1e6
    y_pred_m = y_pred_original / 1e6
    plt.scatter(y_test_m, y_pred_m, alpha=0.5, s=20)
    plt.plot([y_test_m.min(), y_test_m.max()], [y_test_m.min(), y_test_m.max()], 'r--', lw=2)
    plt.xlabel('Actual Views (millions)')
    plt.ylabel('Predicted Views (millions)')
    plt.title('Original Scale Predictions')

    plt.tight_layout()
    plt.show()

    print("\nPerformance Metrics:")
    print(f"Log Scale R²: {r2_score(y_test, y_pred):.4f}")

    ranges = [
        (0, 1e6, '<1M'),
        (1e6, 10e6, '1M-10M'),
        (10e6, 100e6, '10M-100M'),
        (100e6, float('inf'), '>100M')
    ]

if __name__ == "__main__":
    X = data[FEATURES]
    y = np.log1p(data['view_count'])
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model, history, mse, r2, y_pred, best_hps = train_model_with_tuning(X_train, X_test, y_train, y_test)

    y_pred_original = np.expm1(y_pred)
    y_test_original = np.expm1(y_test)

    evaluate_and_visualize(y_test, y_pred, y_test_original, y_pred_original, history)

model.summary()

from IPython.display import SVG
SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))

print("\nFinal Training Metrics:")
print(f"Final training loss: {history.history['loss'][-1]:.4f}")
print(f"Final validation loss: {history.history['val_loss'][-1]:.4f}")
print(f"Best validation loss: {min(history.history['val_loss']):.4f}")
print(f"Number of epochs completed: {len(history.history['loss'])}")

print("\nTest Set Performance:")
print(f"Mean Squared Error (log scale): {mse:.4f}")
print(f"R² Score (log scale): {r2:.4f}")

print("\nBest Hyperparameters:")
for param, value in best_hps.values.items():
    print(f"{param}: {value}")

mse_log = mean_squared_error(y_test, y_pred)
print(f"\nMSE (log scale): {mse_log:.4f}")

y_test_original = np.expm1(y_test)
y_pred_original = np.expm1(y_pred)
mse_original = mean_squared_error(y_test_original, y_pred_original)
print(f"MSE (original scale): {mse_original:.2f}")

rmse_log = np.sqrt(mse_log)
rmse_original = np.sqrt(mse_original)
print(f"RMSE (log scale): {rmse_log:.4f}")
print(f"RMSE (original scale): {rmse_original:.2f}")

print("Shapes:")
print(f"y_test shape: {y_test.shape}")
print(f"y_pred shape: {y_pred.shape}")

y_test_array = np.array(y_test).flatten()
y_pred_array = np.array(y_pred).flatten()

mae_log = np.mean(np.abs(y_test_array - y_pred_array))
y_test_original = np.expm1(y_test_array)
y_pred_original = np.expm1(y_pred_array)
mae_original = np.mean(np.abs(y_test_original - y_pred_original))

print(f"\nMAE (log scale): {mae_log:.4f}")
print(f"MAE (original scale): {mae_original:.2f}")

mape = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100
print(f"\nMAPE: {mape:.2f}%")

y_test_array = np.array(y_test).flatten()
y_pred_array = np.array(y_pred).flatten()

residuals = y_test_array - y_pred_array

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(y_pred_array, residuals, alpha=0.5, s=20)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values (log scale)')
plt.ylabel('Residuals')
plt.title('Residual Plot')

plt.subplot(1, 3, 2)
stats.probplot(residuals, dist="norm", plot=plt)
plt.title('Q-Q Plot')

plt.subplot(1, 3, 3)
plt.hist(residuals, bins=50, density=True, alpha=0.7)
mu, sigma = np.mean(residuals), np.std(residuals)
x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', lw=2,
         label=f'Normal Dist.\nμ={mu:.2f}, σ={sigma:.2f}')
plt.xlabel('Residuals')
plt.ylabel('Density')
plt.title('Error Distribution')
plt.legend()

plt.tight_layout()
plt.show()

print("\nResidual Statistics:")
print(f"Mean of residuals: {np.mean(residuals):.4f}")
print(f"Standard deviation of residuals: {np.std(residuals):.4f}")
print(f"Skewness: {stats.skew(residuals):.4f}")
print(f"Kurtosis: {stats.kurtosis(residuals):.4f}")

y_test_original = np.expm1(y_test_array)
y_pred_original = np.expm1(y_pred_array)

mae_views = np.mean(np.abs(y_test_original - y_pred_original))

print(f"MAE: {mae_views:,.0f} views")

errors = np.abs(y_test_original - y_pred_original)
print("\nError Distribution (views):")
print(f"25th percentile: {np.percentile(errors, 25):,.0f}")
print(f"Median error: {np.percentile(errors, 50):,.0f}")
print(f"75th percentile: {np.percentile(errors, 75):,.0f}")
print(f"90th percentile: {np.percentile(errors, 90):,.0f}")

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.hist(y_test_original, bins=50, alpha=0.7)
plt.title('View Count Distribution')
plt.xlabel('Views (millions)')
plt.ylabel('Frequency')

plt.subplot(1, 3, 2)
plt.hist(y_test_original, bins=50, alpha=0.7, log=True)
plt.title('View Count Distribution (Log Y-axis)')
plt.xlabel('Views (millions)')
plt.ylabel('Frequency (log scale)')

plt.subplot(1, 3, 3)
plt.hist(np.log1p(y_test_original), bins=50, alpha=0.7)
plt.title('Log-Transformed View Distribution')
plt.xlabel('Log(Views in millions + 1)')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

print("\nView Count Statistics:")
print(f"Mean views: {np.mean(y_test_original):,.0f}")
print(f"Median views: {np.median(y_test_original):,.0f}")
print(f"Std deviation: {np.std(y_test_original):,.0f}")
print("\nPercentiles:")
percentiles = [5, 25, 50, 75, 95, 99]
for p in percentiles:
    print(f"{p}th percentile: {np.percentile(y_test_original, p):,.0f} views")

data

